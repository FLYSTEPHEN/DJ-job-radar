{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "def scrape_careers_information(page_url):\n",
    "    r = requests.get(page_url)\n",
    "    data = BeautifulSoup(r.text,\"html.parser\")\n",
    "    jobs= []\n",
    "    all_div = data.find_all('div', attrs={'class':\"bti-ui-job-detail-container\"}) \n",
    "    for i in all_div:\n",
    "        job=[]\n",
    "        job.append(i.find('a').text)\n",
    "        # title\n",
    "        job.append(i.find('div', attrs={'class':\"bti-ui-job-result-detail-employer\"}).text.strip())\n",
    "        # company\n",
    "        job.append(i.find('div', attrs={'class':\"bti-ui-job-result-detail-location\"}).text.strip())\n",
    "        # company address\n",
    "        job.append(i.find('div', attrs={'class':\"bti-ui-job-result-detail-age\"}).text.strip())\n",
    "        # post date\n",
    "        job.append('https://careers.journalists.org{}'.format(i.find('a')['href']))\n",
    "        # page url\n",
    "        job.append('NaN')\n",
    "        # job status\n",
    "        job.append('careers')\n",
    "        # source website\n",
    "        jobs.append(job)\n",
    "    return jobs\n",
    "\n",
    "def scrape_careers_description(description_url):\n",
    "    r = requests.get(description_url)\n",
    "    data = BeautifulSoup(r.text,\"html.parser\")\n",
    "    return data.find('div', attrs={'class':\"bti-jd-description\"}).text\n",
    "\n",
    "def scrape_indeed_information(page_url):\n",
    "    r = requests.get(page_url)\n",
    "    data = BeautifulSoup(r.text,\"html.parser\")\n",
    "    jobs= []\n",
    "    all_h2 = data.find_all('h2', attrs={'class':\"jobtitle\"}) \n",
    "    #it's a weirdo page that the last item's 'class' is different from above 9, so that we use its sub label h2.\n",
    "    for i in all_h2:\n",
    "        job = []\n",
    "        job.append(i.a['title'])\n",
    "        job.append(i.parent.find('span', attrs={'class':\"company\"}).text.strip()) \n",
    "        #use .parent back to the higher label\n",
    "        job.append(i.parent.find('span', attrs={'class':\"location\"}).text.strip())\n",
    "        job.append(i.parent.find('span', attrs={'class':\"date\"}).text.strip())\n",
    "        job.append('https://www.indeed.com/viewjob?jk={}'.format(i['id'][3:]))\n",
    "        job.append('NaN')\n",
    "        job.append('indeed')\n",
    "        if job not in all_jobs:\n",
    "            jobs.append(job)\n",
    "        #when the index url exceeds the range of pages in indeed, the page will become circulation, so that we should do duplicate checking\n",
    "    return jobs\n",
    "\n",
    "def scrape_indeed_description(description_url):\n",
    "    r = requests.get(description_url)\n",
    "    data = BeautifulSoup(r.text,\"html.parser\")\n",
    "    return data.find('div', attrs={'class':\"jobsearch-JobComponent-description icl-u-xs-mt--md\"}).text\n",
    "\n",
    "def scrape_jobsdb_information(page_url):\n",
    "    r = requests.get(page_url)\n",
    "    data = BeautifulSoup(r.text,\"html.parser\")\n",
    "    jobs= []\n",
    "    all_div = data.find_all('div', attrs={'class':\"_3ASfTyv _2EUSthc\"})\n",
    "    for i in all_div:\n",
    "        job=[]\n",
    "        job.append(i.find('div', attrs={'class':\"_3gfm7U9 _3ho-Knb _2swcdgn\"}).a.text)\n",
    "        job.append(i.find('div', attrs={'class':\"_1NdWRqw _3ho-Knb _2swcdgn\"}).find('span').text)\n",
    "        job.append(i.find('div', attrs={'class':\"_124cxoK _3ho-Knb _2swcdgn\"}).find('span').text)\n",
    "        job.append(i.find('span', attrs={'class':\"JG37Vx2 _3Re95QG _2XGgj_O\"}).find('span').text)\n",
    "        job.append(i.find('div', attrs={'class':\"_3gfm7U9 _3ho-Knb _2swcdgn\"}).a['href'])\n",
    "        job.append('NaN')\n",
    "        job.append('jobsdb')\n",
    "        jobs.append(job)\n",
    "    return jobs\n",
    "    \n",
    "def scrape_jobsdb_description(description_url):\n",
    "    r = requests.get(description_url)\n",
    "    data = BeautifulSoup(r.text,\"html.parser\")\n",
    "    return data.find('div', attrs={'class':\"jobad-primary\"}).text\n",
    "\n",
    "def scrape_all_information(base_url,starting_index,step,function_scrape_information):\n",
    "    global all_jobs\n",
    "    all_jobs = [] \n",
    "    page_index = starting_index\n",
    "    while True: \n",
    "        page_url = '{}{}'.format(base_url,page_index)\n",
    "        try:\n",
    "            jobs = function_scrape_information(page_url)\n",
    "        except:\n",
    "            jobs = []   \n",
    "        if jobs == []: #if all jobs have been scraped, break the loop\n",
    "            break\n",
    "        all_jobs.extend(jobs)\n",
    "        if len(all_jobs) > 20: #only scrape 20 jobs from each website\n",
    "            break\n",
    "        page_index += step  #level is in url to indiccate different pages' index\n",
    "#     all_jobs.sort(key=lambda item: item[3], reverse=False) \n",
    "\n",
    "def scrape_all_description(urls_and_sources):\n",
    "    list_descriptions = []\n",
    "    list_websites = [m[\"source\"] for m in websites]\n",
    "    list_functions = [i[\"function_scrape_description\"] for i in websites]\n",
    "    for u, s in urls_and_sources:\n",
    "        list_descriptions.append(list_functions[list_websites.index(s)](u))\n",
    "    return list_descriptions\n",
    "\n",
    "websites = [\n",
    "    {\n",
    "        \"source\": 'careers',\n",
    "        \"base_url\": 'https://careers.journalists.org/jobs/?keywords=data+OR+journalist&page=',\n",
    "        \"starting_index\": 1,        \n",
    "        \"step\": 1,\n",
    "        \"function_scrape_information\": scrape_careers_information,\n",
    "        \"function_scrape_description\": scrape_careers_description,\n",
    "    }, \n",
    "    {\n",
    "        \"source\": 'indeed',\n",
    "        \"base_url\": 'https://www.indeed.com/jobs?q=Data+Journalist+Internship&start=',\n",
    "        \"starting_index\": 0,        \n",
    "        \"step\": 10,\n",
    "        \"function_scrape_information\": scrape_indeed_information,\n",
    "        \"function_scrape_description\": scrape_indeed_description,\n",
    "    },\n",
    "    {\n",
    "        \"source\": 'jobsdb',\n",
    "        \"base_url\": 'https://hk.jobsdb.com/hk/search-jobs/data-journalist/',\n",
    "        \"starting_index\": 1,        \n",
    "        \"step\": 1,\n",
    "        \"function_scrape_information\": scrape_jobsdb_information,\n",
    "        \"function_scrape_description\": scrape_jobsdb_description,\n",
    "    },\n",
    "]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from os import remove\n",
    "try:\n",
    "    e = open('existed-jobs.csv','r')\n",
    "    jobs_existed_url = [row[4] for row in csv.reader(e)]\n",
    "    e.close()\n",
    "except FileNotFoundError:\n",
    "    jobs_existed_url = []\n",
    "try:\n",
    "    remove('new-jobs.csv')\n",
    "except FileNotFoundError:\n",
    "    pass\n",
    "for i in websites:\n",
    "    scrape_all_information(i[\"base_url\"],i[\"starting_index\"],i[\"step\"],i[\"function_scrape_information\"])\n",
    "    e = open('existed-jobs.csv','a')\n",
    "    n = open('new-jobs.csv','a')\n",
    "    for a in all_jobs:\n",
    "        if a[4] not in jobs_existed_url: #duplicate checking\n",
    "            csv.writer(e).writerow(a)\n",
    "            csv.writer(n).writerow(a)\n",
    "    e.close()\n",
    "    n.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- since we scrap the other information from the index page of each website to make the operation quicker. we need go deep into the page of each job to get their description. As a result, we need to read the existed csv first. We also define two functions for each website to scrape a job's description and the other information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "for i in ['existed-jobs.csv','new-jobs.csv']:\n",
    "    try:\n",
    "        df=pd.read_csv(i, header=None, names=['Title','Company','Location','Date','URL','Current Status','Source'])\n",
    "        urls_and_sources = zip(df['URL'].tolist(), df['Source'].tolist())\n",
    "        df['Description'] = scrape_all_description(urls_and_sources)\n",
    "        df.to_csv('{}-with-description.csv'.format(i[:-4]),na_rep='NaN')\n",
    "    except FileNotFoundError:\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
